<!doctype html>
<html lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Centrico LiveLab — End-to-End MLOps — Projects</title>
  <meta name="description" content="Centrico LiveLab: a complete MLOps pipeline demonstrating data engineering, training, inference serving, monitoring with Prometheus + Grafana, and CI quality gates." />
  <link rel="icon" href="/favicon.svg" />
  <link rel="stylesheet" href="/style.css" />
</head>

<body>
  <header class="nav">
    <div class="inner">
      <div class="brand">
        <img src="/logo.svg" alt="Neuromorphic Inference Lab" />
        <span>Neuromorphic Inference Lab</span>
      </div>
      <nav class="navlinks" aria-label="Primary navigation">
        <a data-nav href="/">Home</a>
        <a data-nav class="active" href="/demos/">Projects</a>
        <a data-nav href="/about/">About Me</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="hero">
      <p class="kicker">Project</p>
      <h1 class="h1">Centrico LiveLab — End-to-End MLOps</h1>
      <p class="sub">
        A complete machine learning operations pipeline that takes a model from initial data collection through training, 
        deployment, and ongoing monitoring in production.
      </p>

      <div class="badges">
        <span class="badge">Data Engineering</span>
        <span class="badge">MLOps Pipeline</span>
        <span class="badge">Model Serving</span>
        <span class="badge">Prometheus + Grafana</span>
        <span class="badge">CI Quality Gates</span>
      </div>

      <div style="height:14px"></div>

      <div class="actions">
        <a class="btn primary" href="https://github.com/nepryoon/centrico-livelab-mlops" target="_blank" rel="noopener">Source repository</a>
        <a class="btn" href="/demos/">Back to Projects</a>
      </div>
    </section>

    <section class="section">
      <h2>Overview — For Recruiters and Non-Technical Stakeholders</h2>
      <p class="lead">
        This project demonstrates how to take machine learning models from development into production systems that can run reliably at scale.
      </p>

      <h3 style="margin-top: 24px;">What it does</h3>
      <p>
        Centrico LiveLab is an end-to-end machine learning operations (MLOps) pipeline. It handles everything from ingesting raw data, 
        training predictive models, deploying those models as APIs that can serve predictions, and continuously monitoring system health and model performance.
      </p>

      <h3 style="margin-top: 24px;">What problem it solves</h3>
      <p>
        Most machine learning projects create models that work in isolated notebooks but never make it to production. Moving from a prototype 
        to a reliable production system requires solving problems around data consistency, model versioning, deployment automation, performance 
        monitoring, and quality control. This project addresses all of these concerns in a single integrated system.
      </p>

      <h3 style="margin-top: 24px;">What it demonstrates</h3>
      <p>
        This system shows proficiency in building production-grade machine learning infrastructure. It demonstrates the ability to design and 
        implement complete data pipelines, orchestrate model training workflows, containerize and deploy services, set up monitoring systems, 
        and automate quality checks. These are the core skills needed to operate machine learning systems in real-world production environments.
      </p>

      <p style="margin-top: 16px;">
        The complete system runs locally using Docker, making it verifiable without cloud infrastructure. All components are instrumented for 
        observability, tested automatically, and documented for maintainability.
      </p>
    </section>

    <section class="section">
      <h2>Technical Deep-Dive — For Engineers</h2>
      <p class="lead">
        Detailed architecture, implementation decisions, and technology choices for each layer of the system.
      </p>

      <h3 style="margin-top: 24px;">Purpose and Scope</h3>
      <p>
        This project implements a production-grade MLOps reference architecture demonstrating the complete machine learning lifecycle. 
        The scope covers data ingestion through deployment and monitoring, with emphasis on reproducibility, observability, and automated quality control. 
        The system is designed for local execution using Docker Compose, eliminating cloud dependencies while maintaining production-equivalent patterns.
      </p>

      <h3 style="margin-top: 24px;">Architecture Overview</h3>
      <p>
        The architecture follows a layered approach with clear separation of concerns. Each layer communicates through well-defined interfaces,
        enabling independent development and testing. The data layer handles ingestion and feature engineering. The training layer orchestrates 
        model development with experiment tracking. The serving layer exposes trained models via REST API. The observability layer collects metrics 
        and provides visualization. CI/CD pipelines enforce quality gates across all layers.
      </p>

      <h3 style="margin-top: 24px;">Data Flow and Execution Model</h3>
      <p>
        Data flows unidirectionally from sources through transformation pipelines into feature stores. Training processes read from feature stores
        to ensure consistency between training and inference. Trained models are serialized and versioned in a model registry. The inference service 
        loads models from the registry and serves predictions via REST endpoints. Metrics flow from the API through Prometheus to Grafana dashboards.
        This design ensures reproducibility and enables rollback to any previous model version.
      </p>
    </section>

    <section class="section">
      <h2>Implementation Details and Technology Stack</h2>
      <p class="lead">
        Component-by-component breakdown with technology choices and rationale.
      </p>

      <div class="grid">
        <article class="card col-12">
          <h3>1. Data Engineering Pipeline</h3>
          <p>
            <strong>Implementation:</strong> Python-based ETL pipelines with Pandas for data manipulation and Pydantic for schema validation. 
            Data is ingested from multiple sources (CSV files, databases, REST APIs) and transformed into feature vectors. Schema enforcement 
            occurs at ingestion boundaries to catch data quality issues early.
          </p>
          <p style="margin-top: 8px;">
            <strong>Technology choices:</strong> Pandas was chosen for its mature ecosystem and broad support for data formats. Pydantic provides 
            runtime type checking and automatic validation, catching schema violations before they propagate through the pipeline. Feature stores 
            are implemented using Parquet files for efficient columnar storage and fast read performance during training and inference.
          </p>
          <p style="margin-top: 8px;">
            <strong>Trade-offs:</strong> File-based feature stores sacrifice horizontal scalability for simplicity and zero-cost local operation. 
            For production systems handling high throughput, this would be replaced with Feast or a similar dedicated feature store.
          </p>
          <div class="tags mt-12">
            <span class="tag">Python</span>
            <span class="tag">Pandas</span>
            <span class="tag">Pydantic</span>
            <span class="tag">Parquet</span>
          </div>
        </article>

        <article class="card col-12">
          <h3>2. Training Pipeline and Model Management</h3>
          <p>
            <strong>Implementation:</strong> Scikit-learn for model training with MLflow for experiment tracking and model registry. Each training 
            run logs hyperparameters, metrics, and model artifacts. Models are versioned and tagged (dev, staging, production) to control promotion 
            through environments.
          </p>
          <p style="margin-top: 8px;">
            <strong>Technology choices:</strong> MLflow provides a complete experiment tracking solution with minimal configuration overhead. 
            It integrates with scikit-learn's model serialization and provides a REST API for model retrieval. Scikit-learn was chosen for its 
            simplicity and extensive documentation, though the architecture supports PyTorch or TensorFlow models through MLflow's model flavor abstraction.
          </p>
          <p style="margin-top: 8px;">
            <strong>Trade-offs:</strong> MLflow's file-based backend limits concurrent write performance but enables simple local deployment. 
            Database-backed tracking would be necessary for teams with high training volumes.
          </p>
          <div class="tags mt-12">
            <span class="tag">Python</span>
            <span class="tag">Scikit-learn</span>
            <span class="tag">MLflow</span>
            <span class="tag">Model Registry</span>
          </div>
        </article>

        <article class="card col-12">
          <h3>3. Inference API and Serving</h3>
          <p>
            <strong>Implementation:</strong> FastAPI serves predictions via REST endpoints with automatic OpenAPI documentation. The API loads 
            models from MLflow's model registry at startup and caches them in memory. Request/response schemas are validated using Pydantic models. 
            Health and readiness probes enable Kubernetes-style orchestration.
          </p>
          <p style="margin-top: 8px;">
            <strong>Technology choices:</strong> FastAPI provides automatic request validation, serialization, and interactive API documentation. 
            Its async support enables high-throughput serving. Docker containerization ensures consistent deployment across environments. 
            Gunicorn with Uvicorn workers provides production-grade process management and graceful shutdown handling.
          </p>
          <p style="margin-top: 8px;">
            <strong>Trade-offs:</strong> In-memory model caching requires sufficient RAM for large models. For production systems with memory 
            constraints, models could be loaded on-demand or served through dedicated model servers like TensorFlow Serving or Triton.
          </p>
          <div class="tags mt-12">
            <span class="tag">Python</span>
            <span class="tag">FastAPI</span>
            <span class="tag">Pydantic</span>
            <span class="tag">Docker</span>
            <span class="tag">Gunicorn</span>
            <span class="tag">Uvicorn</span>
          </div>
        </article>

        <article class="card col-12">
          <h3>4. Observability and Monitoring</h3>
          <p>
            <strong>Implementation:</strong> Prometheus client library instruments the FastAPI application with custom metrics (request count, 
            latency histograms, error rates, prediction distributions). Prometheus scrapes these metrics at configured intervals. Grafana reads 
            from Prometheus and renders dashboards showing system health, API performance, and model behavior over time.
          </p>
          <p style="margin-top: 8px;">
            <strong>Technology choices:</strong> Prometheus was chosen for its pull-based architecture, which simplifies network configuration and 
            service discovery. Its time-series database handles high cardinality metrics efficiently. Grafana provides rich visualization capabilities 
            and alerting integration. Both tools are industry standards with extensive community support.
          </p>
          <p style="margin-top: 8px;">
            <strong>Trade-offs:</strong> Prometheus's local storage limits retention periods. For long-term metric storage, integration with 
            remote storage systems like Thanos or Cortex would be required.
          </p>
          <div class="tags mt-12">
            <span class="tag">Prometheus</span>
            <span class="tag">Grafana</span>
            <span class="tag">Prometheus Client</span>
            <span class="tag">Docker Compose</span>
          </div>
        </article>

        <article class="card col-12">
          <h3>5. CI/CD and Quality Gates</h3>
          <p>
            <strong>Implementation:</strong> GitHub Actions workflows run on each commit, executing linters (flake8, black), type checkers (mypy), 
            unit tests (pytest), and model evaluation on hold-out datasets. Quality gates fail the build if code quality metrics or model performance 
            fall below thresholds. Successful builds can trigger automated deployments.
          </p>
          <p style="margin-top: 8px;">
            <strong>Technology choices:</strong> GitHub Actions provides tight integration with the repository and free compute for public repos. 
            Pytest offers parametrized testing and extensive plugin ecosystem. Black and flake8 enforce consistent code style. Mypy catches type 
            errors at CI time rather than runtime.
          </p>
          <p style="margin-top: 8px;">
            <strong>Trade-offs:</strong> GitHub Actions has limited compute resources for free tiers. Resource-intensive training jobs would require 
            self-hosted runners or integration with cloud-based training platforms.
          </p>
          <div class="tags mt-12">
            <span class="tag">GitHub Actions</span>
            <span class="tag">Pytest</span>
            <span class="tag">Flake8</span>
            <span class="tag">Black</span>
            <span class="tag">Mypy</span>
          </div>
        </article>

        <article class="card col-12">
          <h3>6. Infrastructure and Orchestration</h3>
          <p>
            <strong>Implementation:</strong> Docker Compose orchestrates all services (API, MLflow, Prometheus, Grafana) with defined networking 
            and volume mounts. Services communicate via Docker's internal DNS. Configuration is externalized through environment files.
          </p>
          <p style="margin-top: 8px;">
            <strong>Technology choices:</strong> Docker Compose provides declarative multi-container orchestration suitable for development and 
            small-scale deployments. It requires no external dependencies beyond Docker itself. For production, this configuration could be 
            translated to Kubernetes manifests or Helm charts.
          </p>
          <p style="margin-top: 8px;">
            <strong>Trade-offs:</strong> Docker Compose lacks advanced orchestration features (auto-scaling, rolling updates, service mesh). 
            It is appropriate for development environments but would be replaced with Kubernetes for production deployments requiring high availability 
            and horizontal scaling.
          </p>
          <div class="tags mt-12">
            <span class="tag">Docker</span>
            <span class="tag">Docker Compose</span>
            <span class="tag">Linux</span>
          </div>
        </article>
      </div>
    </section>

    <section class="section">
      <h2>Architecture diagram</h2>
      <p class="lead">
        High-level system flow showing data movement from sources through training and serving layers,
        with observability and CI/CD as cross-cutting concerns.
      </p>

      <div class="card">
        <pre style="line-height: 1.6; color: rgba(234,240,255,.86); font-size: 13px; overflow-x: auto;">
┌─────────────────────┐
│  Data Sources       │
│  (CSV, DB, APIs)    │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Data Pipeline      │
│  • Ingestion        │
│  • Validation       │
│  • Feature Eng      │
└──────────┬──────────┘
           │
           ├──────────────────────┐
           │                      │
           ▼                      ▼
┌─────────────────────┐  ┌─────────────────────┐
│  Training Pipeline  │  │  Feature Store      │
│  • MLflow           │  │  (Inference Ready)  │
│  • Model Registry   │  └──────────┬──────────┘
└──────────┬──────────┘             │
           │                        │
           ▼                        ▼
┌─────────────────────┐  ┌─────────────────────┐
│  Model Artefacts    │  │  Inference API      │
│  (Versioned)        │─▶│  • FastAPI          │
└─────────────────────┘  │  • Health Checks    │
                         └──────────┬──────────┘
                                    │
                                    ▼
                         ┌─────────────────────┐
                         │  Monitoring Stack   │
                         │  • Prometheus       │
                         │  • Grafana          │
                         │  • Alerting         │
                         └─────────────────────┘

Cross-Cutting: CI/CD pipeline with quality gates at each stage
        </pre>
      </div>
    </section>

    <section class="section">
      <h2>Technical Challenges and Solutions</h2>
      <p class="lead">
        Key engineering problems encountered and solutions implemented.
      </p>

      <div class="grid">
        <div class="card col-6">
          <h3>Data Consistency</h3>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Challenge:</strong> Ensuring training and inference use identical feature transformations.
          </p>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Solution:</strong> Centralized feature engineering logic with Pydantic schemas enforced at pipeline boundaries. 
            Feature generation code is versioned alongside models to maintain consistency.
          </p>
        </div>

        <div class="card col-6">
          <h3>Model Versioning</h3>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Challenge:</strong> Tracking which model version is deployed and enabling rollbacks.
          </p>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Solution:</strong> MLflow model registry with explicit stage transitions (dev → staging → production). 
            Each model artifact includes training metadata and lineage information.
          </p>
        </div>

        <div class="card col-6">
          <h3>Observability</h3>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Challenge:</strong> Detecting model performance degradation in production.
          </p>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Solution:</strong> Comprehensive instrumentation capturing request patterns, latency distributions, and prediction statistics. 
            Grafana dashboards provide real-time visibility into model behavior.
          </p>
        </div>

        <div class="card col-6">
          <h3>Quality Control</h3>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Challenge:</strong> Preventing poorly-performing models from reaching production.
          </p>
          <p style="margin-top: 8px; font-size: 14px; line-height: 1.6;">
            <strong>Solution:</strong> Automated CI gates evaluating model performance on hold-out data. Builds fail if accuracy, precision, 
            or recall fall below defined thresholds.
          </p>
        </div>
      </div>
    </section>

    <section class="section">
      <h2>Local Setup and Verification</h2>
      <p class="lead">
        The system can be run entirely locally using Docker Compose, enabling developers to test
        and verify the complete stack without cloud dependencies.
      </p>

      <div class="grid">
        <div class="card col-6">
          <h3>Quick start</h3>
          <ol class="clean" style="padding-left: 20px;">
            <li>Clone the repository: <code style="font-size: 12px; padding: 2px 6px; background: rgba(0,0,0,.3); border-radius: 4px;">git clone https://github.com/nepryoon/centrico-livelab-mlops</code></li>
            <li>Run setup: <code style="font-size: 12px; padding: 2px 6px; background: rgba(0,0,0,.3); border-radius: 4px;">docker-compose up</code></li>
            <li>Access services:
              <ul style="margin-top: 6px;">
                <li>API docs: <code style="font-size: 12px; padding: 2px 6px; background: rgba(0,0,0,.3); border-radius: 4px;">http://localhost:8000/docs</code></li>
                <li>Grafana: <code style="font-size: 12px; padding: 2px 6px; background: rgba(0,0,0,.3); border-radius: 4px;">http://localhost:3000</code></li>
                <li>Prometheus: <code style="font-size: 12px; padding: 2px 6px; background: rgba(0,0,0,.3); border-radius: 4px;">http://localhost:9090</code></li>
              </ul>
            </li>
          </ol>
        </div>

        <div class="card col-6">
          <h3>Verification checklist</h3>
          <ul class="clean">
            <li>✓ Data pipeline runs without errors</li>
            <li>✓ Training produces versioned model artefacts</li>
            <li>✓ Inference API responds to health checks</li>
            <li>✓ Prometheus collects metrics from API</li>
            <li>✓ Grafana dashboards display system metrics</li>
            <li>✓ CI pipeline passes all quality gates</li>
          </ul>
        </div>
      </div>
    </section>

    <footer class="footer">
      <div>© 2025 Neuromorphic Inference Lab</div>
      <div class="prov">
        <span id="build-branch">branch: …</span>
        <span id="build-commit">commit: …</span>
        <span id="build-time">built: …</span>
      </div>
    </footer>
  </main>

  <script src="/build-info.js"></script>
</body>
</html>