<!doctype html>
<html lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NeuroGrid Fault Risk Scoring Platform — Projects</title>
  <meta name="description" content="End-to-end ML system: feature engineering, MLflow training, CI/CD for ML, artefact versioning, FastAPI serving, and monitoring-ready outputs." />
  <link rel="icon" href="/favicon.svg" />
  <link rel="stylesheet" href="/style.css" />
</head>

<body>
  <header class="nav">
    <div class="inner">
      <div class="brand">
        <img src="/logo.svg" alt="Neuromorphic Inference Lab" />
        <span>Neuromorphic Inference Lab</span>
      </div>
      <nav class="navlinks" aria-label="Primary navigation">
        <a data-nav href="/">Home</a>
        <a data-nav class="active" href="/">Projects</a>
        <a data-nav href="/about/">About Me</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="hero">
      <p class="kicker">Predictive Maintenance</p>
      <h1 class="h1">NeuroGrid Fault Risk Scoring Platform</h1>
      <p class="sub">
        A production ML system that predicts electrical grid fault risk to prioritise preventive maintenance.
        Demonstrates end-to-end ML delivery: feature engineering, model training, CI/CD pipelines, versioned artefacts, and API serving.
      </p>

      <div class="actions">
        <a class="btn primary" href="https://mv-grid-fault-risk-api.onrender.com/docs" target="_blank" rel="noopener">Live API docs</a>
        <a class="btn" href="https://github.com/nepryoon/mv-grid-fault-risk" target="_blank" rel="noopener">Source</a>
      </div>

      <div class="badges">
        <span class="badge">CI/CD for ML</span>
        <span class="badge">Feature Engineering</span>
        <span class="badge">MLflow Tracking</span>
        <span class="badge">FastAPI Serving</span>
        <span class="badge">Docker Deployment</span>
      </div>
    </section>

    <section class="section">
      <article class="card project">
        <h2>What This Project Does</h2>
        <p>
          This system predicts the probability of electrical faults on medium-voltage power grid assets over the next 30 days. It produces a risk score and classification (LOW, MEDIUM, HIGH, CRITICAL) to help maintenance teams decide which equipment needs attention first.
        </p>
        <p>
          The problem it solves: electrical utilities manage thousands of assets (transformers, cables, circuit breakers) and need a data-driven way to prioritise inspections and repairs. Without this, maintenance is reactive—teams respond to failures after they occur, leading to outages, safety issues, and higher costs. This system enables proactive maintenance by identifying high-risk assets before they fail.
        </p>
        <p>
          What it demonstrates: this is a complete ML system, not just a model. It shows how to take a business problem through the full ML lifecycle—data preparation, feature engineering, model training with experiment tracking (MLflow), automated testing, versioned model artefacts, containerised deployment, and a production API that other systems can integrate with. The live demo below connects to a real deployed service.
        </p>
      </article>

      <article class="card project">
        <h2>Technical Deep Dive</h2>
        
        <h3>Purpose and Scope</h3>
        <p>
          This system implements a supervised classification model for fault risk prediction on medium-voltage electrical grid assets. The model consumes historical fault records, asset metadata, and time-based features to output a probability estimate and risk classification. The scope encompasses the entire ML pipeline: feature engineering, model training, experiment tracking, CI/CD for model deployment, and inference serving via REST API.
        </p>

        <h3>Architecture and Design Decisions</h3>
        <p>
          The architecture follows a modular pipeline pattern: ingestion → feature engineering → training → evaluation → artefact versioning → serving. Feature engineering transforms raw operational data (fault timestamps, asset attributes) into time-windowed aggregates (faults in last 30/90/180 days) and categorical encodings. The training module uses MLflow for experiment tracking and model registry, enabling reproducible experiments and version-controlled artefacts.
        </p>
        <p>
          The serving layer separates model execution from the API interface: a FastAPI application loads the serialised model artefact at startup and exposes a <code>/predict</code> endpoint. This design allows model updates without changing API contracts. The system is deployed in Docker containers on Render, with a Cloudflare proxy for same-origin requests from this site.
        </p>

        <h3>Data Flow and Execution Model</h3>
        <p>
          <strong>Training flow:</strong> Raw fault logs → time-based feature extraction (days since last fault, rolling counts) → categorical feature encoding (asset type, region, manufacturer) → train/test split using time-based cutoff (prevents data leakage) → model training with cross-validation → MLflow logging (metrics, parameters, artefacts) → model serialisation to <code>.pkl</code> with versioned release tag.
        </p>
        <p>
          <strong>Inference flow:</strong> Client sends JSON payload with feature values → FastAPI validates schema (Pydantic models) → model predicts fault probability → post-processing applies risk band thresholds (0-0.3: LOW, 0.3-0.6: MEDIUM, 0.6-0.8: HIGH, 0.8+: CRITICAL) → API returns JSON response with probability, risk band, and model metadata.
        </p>

        <h3>Technology Stack and Rationale</h3>
        <p>
          <strong>Python 3.11:</strong> Core language for ML and API. Chosen for mature ML ecosystem (scikit-learn, pandas) and type safety improvements (used with Pydantic for runtime validation).
        </p>
        <p>
          <strong>scikit-learn:</strong> Model implementation. Provides production-ready classifiers (RandomForestClassifier used here), pipeline abstractions for feature transformations, and serialisation. Chosen over deep learning frameworks because the problem is tabular with limited data—simpler models generalise better and are easier to interpret.
        </p>
        <p>
          <strong>MLflow:</strong> Experiment tracking and model registry. Logs hyperparameters, metrics (precision, recall, F1), and model artefacts for reproducibility. Enables comparison across training runs and promotes models from experimentation to production. Alternative (Weights & Biases) requires external service; MLflow can run self-hosted.
        </p>
        <p>
          <strong>FastAPI:</strong> REST API framework. Automatic OpenAPI/Swagger documentation, request validation with Pydantic, async support (though not required here), and fast startup times. Chosen over Flask for better type safety and built-in docs. Chosen over Django for lower overhead—this is an inference service, not a full web application.
        </p>
        <p>
          <strong>Pydantic:</strong> Data validation. Enforces schema at API boundary (input features must match expected types and ranges). Catches bad requests before they reach the model, preventing runtime errors and nonsensical predictions.
        </p>
        <p>
          <strong>Docker:</strong> Containerisation. Ensures consistent environment between local development, CI, and production. The <code>Dockerfile</code> uses multi-stage builds to keep image size small (base Python + dependencies layer, then add application code).
        </p>
        <p>
          <strong>GitHub Actions:</strong> CI/CD pipeline. Automates model training on push to main, runs tests, builds Docker image, and creates versioned releases with model artefacts. This ensures every model in production is traceable to a specific commit and training run.
        </p>
        <p>
          <strong>Render:</strong> Deployment platform. Hosts the Dockerised FastAPI service. Chosen for simplicity (auto-deploy from GitHub) and free tier for demos. Limitation: cold starts on free tier cause 30-60s delay on first request after idle period.
        </p>
        <p>
          <strong>Cloudflare Workers:</strong> Proxy layer. Routes requests from this static site to the Render service, avoiding CORS issues. Adds caching and DDoS protection.
        </p>

        <h3>Implementation Details</h3>
        <p>
          Feature engineering: time-based features use <code>pandas</code> date arithmetic to compute days since last event and rolling window counts. Categorical features are one-hot encoded with <code>sklearn.preprocessing.OneHotEncoder</code>. Missing values are imputed with median (numeric) or mode (categorical) to handle incomplete records.
        </p>
        <p>
          Model: <code>RandomForestClassifier</code> with 100 trees, max depth 10, and class weighting to handle imbalanced classes (faults are rare events). Hyperparameters (n_estimators: 50-200, max_depth: 5-15, min_samples_split: 2-10) were tuned via grid search logged in MLflow. The model outputs calibrated probabilities using <code>CalibratedClassifierCV</code> to ensure probability estimates are reliable for decision-making.
        </p>
        <p>
          Testing: unit tests verify feature engineering logic, integration tests check API responses, and smoke tests validate the deployed service. CI runs tests on every commit before building the Docker image.
        </p>
        <p>
          Versioning: model artefacts are tagged with semantic versions (<code>model-v0.1.0</code>) and stored in GitHub Releases. The API reads the model version from a manifest file packaged in the Docker image, exposing it in the <code>/health</code> endpoint for observability.
        </p>

        <h3>Technical Challenges and Trade-offs</h3>
        <p>
          <strong>Data leakage prevention:</strong> Used time-based train/test split instead of random split. Random splitting would leak future information into training set (a fault on day 100 influences features for day 99), inflating metrics. Time-based split mirrors real deployment: model trained on past data, evaluated on future data.
        </p>
        <p>
          <strong>Class imbalance:</strong> Fault events are rare (~5% positive class). Standard classifiers overpredict the majority class. Solution: class weighting (<code>class_weight='balanced'</code>) and threshold tuning. Trade-off: higher recall (catch more faults) at the cost of precision (more false alarms). Acceptable because false alarm cost (unnecessary inspection) is lower than miss cost (unplanned outage).
        </p>
        <p>
          <strong>Cold start latency:</strong> Render free tier spins down after inactivity. First request after idle takes 30-60s to wake up. Mitigation: health check pre-flight request in the demo form. Alternative (paid tier with always-on instances) eliminates this but adds cost. Acceptable trade-off for a demo project.
        </p>
        <p>
          <strong>Model interpretability vs. performance:</strong> Random forests are less interpretable than logistic regression but provide better predictions on this data (non-linear relationships between features). For production deployment, added SHAP value logging (not exposed in this demo) to explain individual predictions. Trade-off: complexity for accuracy, mitigated with post-hoc explainability.
        </p>
        <p>
          <strong>Deployment simplicity vs. scalability:</strong> Single-instance deployment on Render. Sufficient for demo load (~10 req/min) but would require horizontal scaling (multiple instances behind load balancer) for production traffic. Future improvement: migrate to Kubernetes or serverless (AWS Lambda) for auto-scaling. Current design prioritises ease of setup over scale.
        </p>
      </article>

      <article class="card project">
        <h2>Live Inference Demo</h2>
        <p>
          Score risk via a same-origin Cloudflare proxy (no CORS). If the upstream service was idle, the first request may take a moment to warm up.
        </p>

        <div class="card project p-16">
            <form id="score-form">
              <div class="grid">
                <div class="col-6">
                  <label><span>Asset age (years)</span>
                    <input name="asset_age_years" type="number" step="0.5" value="15" />
                  </label>
                </div>
                <div class="col-6">
                  <label><span>Days since last fault</span>
                    <input name="days_since_last_fault" type="number" step="1" value="365" />
                  </label>
                </div>

                <div class="col-4">
                  <label><span>Faults last 30 days</span>
                    <input name="faults_last_30d" type="number" step="1" value="0" />
                  </label>
                </div>
                <div class="col-4">
                  <label><span>Faults last 90 days</span>
                    <input name="faults_last_90d" type="number" step="1" value="1" />
                  </label>
                </div>
                <div class="col-4">
                  <label><span>Faults last 180 days</span>
                    <input name="faults_last_180d" type="number" step="1" value="2" />
                  </label>
                </div>

                <div class="col-6">
                  <label><span>Asset type</span>
                    <select name="asset_type">
                      <option value="overhead" selected>overhead</option>
                      <option value="underground">underground</option>
                      <option value="substation">substation</option>
                      <option value="unknown">unknown</option>
                    </select>
                  </label>
                </div>

                <div class="col-6">
                  <label><span>Region</span>
                    <select name="region">
                      <option value="north" selected>north</option>
                      <option value="centre">centre</option>
                      <option value="south">south</option>
                      <option value="unknown">unknown</option>
                    </select>
                  </label>
                </div>

                <div class="col-6">
                  <label><span>Voltage (kV)</span>
                    <input name="voltage_kv" type="number" step="1" value="15" />
                  </label>
                </div>

                <div class="col-6">
                  <label><span>Manufacturer</span>
                    <input name="manufacturer" type="text" value="GenericGridCo" />
                  </label>
                </div>

                <div class="col-12 mt-6">
                  <button class="btn primary" type="submit">Score risk</button>
                  <a class="btn" href="https://mv-grid-fault-risk-api.onrender.com/docs" target="_blank" rel="noopener">Open Swagger</a>
                </div>
              </div>
            </form>

            <div id="result" class="mt-14 muted"></div>
          </div>
      </article>
    </section>

    <footer class="footer">
      <div>© 2025 Neuromorphic Inference Lab</div>
      <div class="prov">
        <span id="build-branch">branch: …</span>
        <span id="build-commit">commit: …</span>
        <span id="build-time">built: …</span>
      </div>
    </footer>
  </main>

  <script>

    const API_PREDICT = "/api/mvgrid/predict";
    const API_HEALTH  = "/api/mvgrid/health";

    function toNumber(v) {
      const n = Number(v);
      return Number.isFinite(n) ? n : v;
    }

    function pillRow(items) {
      return `<div class="prov">${items.map(x => `<span>${x}</span>`).join("")}</div>`;
    }

    document.getElementById("score-form").addEventListener("submit", async (e) => {
      e.preventDefault();
      const form = e.target;
      const data = new FormData(form);

      const features = {};
      for (const [k, v] of data.entries()) {
        const vv = (typeof v === "string") ? v.trim() : v;
        features[k] = vv === "" ? null : toNumber(vv);
      }

      const resultEl = document.getElementById("result");
      resultEl.textContent = "Scoring… (If the service was idle, it may take a moment to warm up.)";

      try { await fetch(API_HEALTH, { cache: "no-store" }); } catch (_) {}

      try {
        const res = await fetch(API_PREDICT, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ features })
        });

        const txt = await res.text();
        if (!res.ok) throw new Error(`API error (${res.status}): ${txt}`);

        const out = JSON.parse(txt);
        const prob = Number(out.fault_probability_30d);
        const band = String(out.risk_band || "unknown").toUpperCase();
        const model = String(out.model_source || "model");

        resultEl.innerHTML =
          pillRow([
            `fault probability (30d): <strong>${Number.isFinite(prob) ? prob.toFixed(3) : out.fault_probability_30d}</strong>`,
            `risk band: <strong>${band}</strong>`,
            `model: ${model}`
          ]) + `<div class="mt-10"><small>Live inference via Cloudflare proxy → Render FastAPI → versioned model artefact.</small></div>`;

      } catch (err) {
        resultEl.textContent = "Request failed: " + err.message;
      }
    });
  </script>

  <script src="/build-info.js"></script>
</body>
</html>
