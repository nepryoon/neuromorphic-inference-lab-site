<!doctype html>
<html lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>RAG Knowledge Copilot — Neuromorphic Inference Lab</title>
  <meta name="description" content="RAG Knowledge Copilot: retrieval tracing, grounded drafting, guardrails, and evaluation harness — a Cloudflare-safe, client-only LLMOps demo." />
  <link rel="icon" href="/favicon.svg" />
  <link rel="stylesheet" href="/style.css" />

  <style>
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
    .muted { color: rgba(234,240,255,.74); }
    .tiny { font-size: 12px; line-height: 1.55; color: rgba(234,240,255,.72); }
    .row { display: flex; flex-wrap: wrap; gap: 10px; align-items: center; }
    .stack { display: grid; grid-template-columns: 1fr; gap: 12px; }
    @media (min-width: 980px){ .stack { grid-template-columns: 1.25fr .75fr; } }
    .field { display: grid; gap: 8px; }
    .input, .textarea, .select {
      width: 100%;
      border-radius: 14px;
      border: 1px solid rgba(255,255,255,.10);
      background: rgba(255,255,255,.04);
      color: rgba(234,240,255,.92);
      padding: 12px 12px;
      outline: none;
    }
    .textarea { min-height: 120px; resize: vertical; }
    .input::placeholder, .textarea::placeholder { color: rgba(234,240,255,.45); }
    .panel {
      border: 1px solid rgba(255,255,255,.10);
      background: rgba(255,255,255,.03);
      border-radius: 18px;
      padding: 14px;
    }
    .policy {
      display: grid;
      grid-template-columns: 1fr;
      gap: 10px;
    }
    .policy label {
      display: flex;
      gap: 10px;
      align-items: flex-start;
      padding: 10px;
      border-radius: 14px;
      border: 1px solid rgba(255,255,255,.08);
      background: rgba(255,255,255,.03);
      cursor: pointer;
    }
    .policy input { margin-top: 3px; }
    .hr { border: 0; border-top: 1px solid rgba(255,255,255,.10); margin: 12px 0; }
    details.trace > summary {
      cursor: pointer;
      list-style: none;
      user-select: none;
      font-weight: 600;
    }
    details.trace > summary::-webkit-details-marker { display: none; }
    .traceList { display: grid; gap: 10px; margin-top: 10px; }
    .hit {
      border: 1px solid rgba(255,255,255,.10);
      background: rgba(0,0,0,.18);
      border-radius: 14px;
      padding: 10px;
    }
    .hitTop {
      display: flex; justify-content: space-between; gap: 10px; flex-wrap: wrap;
      align-items: baseline;
    }
    .scoreBar {
      width: 160px;
      height: 10px;
      border-radius: 999px;
      border: 1px solid rgba(255,255,255,.10);
      background: rgba(255,255,255,.06);
      overflow: hidden;
    }
    .scoreFill {
      height: 100%;
      width: 0%;
      background: linear-gradient(90deg, rgba(88,211,255,.75), rgba(161,120,255,.75));
    }
    .kbd{
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 12px;
      padding: 2px 7px;
      border-radius: 10px;
      border: 1px solid rgba(255,255,255,.12);
      background: rgba(0,0,0,.20);
      color: rgba(234,240,255,.88);
    }
    .toast {
      position: fixed;
      left: 50%;
      transform: translateX(-50%);
      bottom: 18px;
      padding: 10px 12px;
      border-radius: 14px;
      border: 1px solid rgba(255,255,255,.12);
      background: rgba(0,0,0,.55);
      color: rgba(234,240,255,.92);
      font-size: 13px;
      opacity: 0;
      pointer-events: none;
      transition: opacity .2s ease;
      max-width: min(720px, 92vw);
    }
    .toast.show { opacity: 1; }

    /* Eval harness */
    .pill {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 6px 10px;
      border-radius: 999px;
      border: 1px solid rgba(255,255,255,.12);
      background: rgba(255,255,255,.04);
      font-size: 12px;
      color: rgba(234,240,255,.86);
    }
    .pill b { font-weight: 700; }
    .evalGrid { display: grid; grid-template-columns: 1fr; gap: 10px; }
    .evalTableWrap {
      overflow-x: auto;
      border: 1px solid rgba(255,255,255,.10);
      border-radius: 16px;
      background: rgba(0,0,0,.12);
    }
    table.eval {
      width: 100%;
      border-collapse: collapse;
      min-width: 760px;
      font-size: 12px;
    }
    table.eval th, table.eval td {
      border-bottom: 1px solid rgba(255,255,255,.08);
      padding: 10px 10px;
      text-align: left;
      vertical-align: top;
      color: rgba(234,240,255,.84);
    }
    table.eval th {
      position: sticky;
      top: 0;
      background: rgba(10,12,18,.85);
      backdrop-filter: blur(8px);
      z-index: 1;
      color: rgba(234,240,255,.92);
    }
    .ok { color: rgba(138,255,186,.95); font-weight: 700; }
    .bad { color: rgba(255,130,130,.95); font-weight: 700; }
    .warn { color: rgba(255,211,120,.95); font-weight: 700; }
  </style>
</head>

<body>
  <header class="nav">
    <div class="inner">
      <div class="brand">
        <img src="/logo.svg" alt="Neuromorphic Inference Lab" />
        <span>Neuromorphic Inference Lab</span>
      </div>
      <nav class="navlinks" aria-label="Primary navigation">
        <a data-nav href="/">Signal</a>
        <a data-nav class="active" href="/demos/">Systems</a>
        <a data-nav href="/evidence/">Proof Ledger</a>
        <a data-nav href="/about/">Identity</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="hero">
      <p class="kicker">System</p>
      <h1 class="h1">RAG Knowledge Copilot</h1>
      <p class="sub">
        A client-only RAG demo that surfaces production signals: <strong>retrieval tracing</strong>, <strong>grounded drafting</strong>,
        <strong>guardrails</strong>, and a lightweight <strong>evaluation harness</strong>.
        Designed to be reliable on Cloudflare Pages (no backend calls, no CORS failure modes).
      </p>

      <div class="badges">
        <span class="badge">RAG</span>
        <span class="badge">LLMOps</span>
        <span class="badge">Retrieval Tracing</span>
        <span class="badge">Guardrails</span>
        <span class="badge">Evaluation Harness</span>
      </div>

      <div style="height:12px"></div>

      <div class="tags" aria-label="Breadcrumb">
        <a class="tag" href="/">Signal</a>
        <span class="tag">→</span>
        <a class="tag" href="/demos/">Systems</a>
        <span class="tag">→</span>
        <a class="tag" href="/evidence/">Proof Ledger</a>
        <span class="tag">→</span>
        <a class="tag" href="/evidence/rag-copilot/">Evidence memo</a>
      </div>

      <div style="height:14px"></div>

      <div class="actions">
        <a class="btn" href="/evidence/#rag-copilot">Proof anchor</a>
        <a class="btn" href="/evidence/rag-copilot/">Open memo</a>
        <a class="btn primary" href="/demos/">Systems index</a>
      </div>
    </section>

    <section class="section">
      <div class="stack">

        <!-- Main interaction -->
        <article class="card">
          <h2>Ask a question</h2>
          <p class="lead">
            This demo drafts only from retrieved sources. If evidence is weak, it refuses (Strict policy).
          </p>

          <div class="field">
            <label class="tiny" for="q">Query</label>
            <input id="q" class="input" type="text" placeholder="e.g. What should be monitored for deployed ML models? Provide citations." />
          </div>

          <div class="row" style="margin-top:10px;">
            <button id="run" class="btn primary" type="button">Run retrieval + draft</button>
            <button id="clear" class="btn" type="button">Clear</button>
            <button id="copyTrace" class="btn" type="button">Export trace JSON</button>
            <button id="copyCites" class="btn" type="button">Copy citations</button>
          </div>

          <div class="hr"></div>

          <div class="field">
            <label class="tiny">Answer (grounded draft)</label>
            <div id="answer" class="panel">
              <p class="muted" style="margin:0;">
                Run a query to see a grounded draft with citations. Use <span class="kbd">Strict</span> to enforce refusals on weak evidence.
              </p>
            </div>
          </div>

          <div style="height:10px"></div>

          <details class="trace" id="traceBox">
            <summary>Retrieval trace (top-k sources, similarity, snippets)</summary>
            <div id="trace" class="traceList"></div>
          </details>

          <div class="hr"></div>

          <h2>Evaluation harness</h2>
          <p class="tiny">
            A lightweight, deterministic evaluation set for screening. Tests include positive queries (should retrieve relevant sources)
            and negative queries (should refuse in Strict mode).
          </p>

          <div class="row" style="margin:10px 0;">
            <button id="runEvalStrict" class="btn primary" type="button">Run evaluation (Strict)</button>
            <button id="runEvalLenient" class="btn" type="button">Run evaluation (Lenient)</button>
            <button id="copyEval" class="btn" type="button">Export eval report JSON</button>
          </div>

          <div class="row" style="margin:10px 0;">
            <span class="pill"><b id="evalScore">—</b> overall score</span>
            <span class="pill"><b id="evalRefusal">—</b> refusal correctness</span>
            <span class="pill"><b id="evalRetrieval">—</b> retrieval@k</span>
            <span class="pill"><b id="evalCitations">—</b> citation presence</span>
            <span class="pill"><b id="evalTrace">—</b> trace completeness</span>
          </div>

          <div class="evalTableWrap">
            <table class="eval" aria-label="Evaluation results">
              <thead>
                <tr>
                  <th style="width: 190px;">Test</th>
                  <th>Query</th>
                  <th style="width: 120px;">Expected</th>
                  <th style="width: 110px;">Refusal</th>
                  <th style="width: 120px;">Retrieval@k</th>
                  <th style="width: 120px;">Citations</th>
                  <th style="width: 140px;">Top similarity</th>
                  <th style="width: 160px;">Notes</th>
                </tr>
              </thead>
              <tbody id="evalBody">
                <tr>
                  <td colspan="8" class="muted">Run an evaluation to populate results.</td>
                </tr>
              </tbody>
            </table>
          </div>

          <p class="tiny" style="margin-top:10px;">
            Interpretation: this is not a benchmark against other LLMs; it demonstrates <strong>production evaluation habits</strong>
            (refusal correctness, retrieval quality proxies, traceability).
          </p>
        </article>

        <!-- Right panel -->
        <aside class="card">
          <h2>Policy & examples</h2>
          <p class="tiny">
            Choose how strict the system should be when evidence is weak. This models real LLMOps trade-offs.
          </p>

          <div class="policy" role="radiogroup" aria-label="Policy mode">
            <label>
              <input type="radio" name="policy" value="strict" checked />
              <div>
                <strong>Strict (recommended)</strong><br />
                <span class="tiny">Refuses if top similarity is below threshold. Optimised for reliability and auditability.</span>
              </div>
            </label>

            <label>
              <input type="radio" name="policy" value="lenient" />
              <div>
                <strong>Lenient</strong><br />
                <span class="tiny">Drafts a cautious answer even with weaker retrieval, but still cites sources and flags uncertainty.</span>
              </div>
            </label>
          </div>

          <div class="hr"></div>

          <h3>Example queries</h3>
          <div class="tags" aria-label="Example queries">
            <button class="tag" data-example="What should be monitored for deployed ML models? Provide citations.">Model monitoring</button>
            <button class="tag" data-example="Explain CI/CD for ML deployment and why model artefact versioning matters. Cite sources.">CI/CD for ML</button>
            <button class="tag" data-example="What is data drift vs prediction drift, and how do you detect them? Provide citations.">Drift detection</button>
            <button class="tag" data-example="Describe a safe RAG workflow with guardrails against prompt injection. Cite sources.">RAG guardrails</button>
            <button class="tag" data-example="What is a feature store and when would you use one? Provide citations.">Feature store</button>
          </div>

          <p class="tiny" style="margin-top:10px;">
            ATS keywords surfaced here: <span class="kbd">RAG</span>, <span class="kbd">LLMOps</span>, <span class="kbd">CI/CD for ML</span>,
            <span class="kbd">Model Serving</span>, <span class="kbd">Monitoring</span>, <span class="kbd">Drift</span>.
          </p>

          <div class="hr"></div>

          <h3>Local corpus</h3>
          <p class="tiny">
            Retrieval runs over a small curated corpus embedded in this page. In production, the same interface maps to a vector store.
          </p>

          <div class="actions">
            <a class="btn" href="/evidence/rag-copilot/">Read the evidence memo</a>
            <a class="btn" href="/evidence/#rag-copilot">Go to proof anchor</a>
          </div>
        </aside>
      </div>
    </section>

    <footer class="footer">
      <div>© <span id="year"></span> Neuromorphic Inference Lab</div>
      <div class="prov">
        <span id="build-branch">branch: …</span>
        <span id="build-commit">commit: …</span>
        <span id="build-time">built: …</span>
      </div>
    </footer>
  </main>

  <div id="toast" class="toast" role="status" aria-live="polite"></div>

  <script>
    document.getElementById("year").textContent = String(new Date().getFullYear());
  </script>
  <script src="/build-info.js"></script>

  <script>
    // -----------------------------
    // RAG Copilot (client-only)
    // -----------------------------

    const CORPUS = [
      {
        id: "doc-ml-monitoring",
        title: "Model Monitoring: What to Track in Production",
        source: "Neuromorphic Inference Lab (internal memo)",
        url: "/evidence/#rag-copilot",
        text: `
Monitoring deployed ML requires both engineering and modelling signals. Track service health (latency, throughput, error rates),
model performance proxies (prediction distribution shift), and delayed ground-truth performance where available.
Key concepts include data drift vs prediction drift, feature schema validation, and alerting tied to business impact.
Operationally, define retraining triggers, rollback criteria, and monitoring dashboards for reliability.
        `.trim()
      },
      {
        id: "doc-cicd-ml",
        title: "CI/CD for ML: Artefacts, Reproducibility, and Safe Releases",
        source: "Neuromorphic Inference Lab (internal memo)",
        url: "/evidence/#mv-grid-fault-risk",
        text: `
CI/CD for ML extends software delivery with model-specific gates: data validation, evaluation checks, and artefact versioning.
A safe release pipeline stores trained models as versioned artefacts, captures lineage metadata, and supports rollbacks.
Automated tests include schema checks, unit tests for transforms, and regression checks on evaluation metrics.
        `.trim()
      },
      {
        id: "doc-drift",
        title: "Drift: Data Drift vs Prediction Drift",
        source: "Neuromorphic Inference Lab (internal memo)",
        url: "/evidence/#forecast-studio",
        text: `
Data drift describes changes in input feature distributions. Prediction drift describes changes in model outputs over time.
Both can be monitored using statistical tests, distance metrics, and distribution summaries.
Drift alerts should be paired with remediation: investigate pipeline changes, update feature engineering, retrain or recalibrate.
        `.trim()
      },
      {
        id: "doc-feature-store",
        title: "Feature Stores: When and Why",
        source: "Neuromorphic Inference Lab (internal memo)",
        url: "/evidence/#mv-grid-fault-risk",
        text: `
A feature store is used when consistent feature computation is needed across training and serving, especially at scale.
It improves reproducibility, reduces training-serving skew, and enables feature reuse across teams and models.
Feature stores also support governance: lineage, access control, and monitoring of feature freshness.
        `.trim()
      },
      {
        id: "doc-rag-guardrails",
        title: "RAG Guardrails: Grounding, Refusal, and Prompt Injection Defence",
        source: "Neuromorphic Inference Lab (internal memo)",
        url: "/evidence/#rag-copilot",
        text: `
Reliable RAG requires explicit grounding: answers should be constrained to retrieved evidence.
If retrieval confidence is weak, the system should refuse or ask clarifying questions.
Prompt injection defence includes treating retrieved text as untrusted data, applying allow-list behaviour, and auditing traces.
Evaluation harnesses measure precision@k, citation correctness, and refusal correctness.
        `.trim()
      },
      {
        id: "doc-evaluation",
        title: "Evaluation Mindset: Beyond a Single Score",
        source: "Neuromorphic Inference Lab (internal memo)",
        url: "/evidence/#forecast-studio",
        text: `
Production evaluation is continuous. Use backtesting for forecasting, and structured evaluation datasets for LLM workflows.
Track performance across slices, horizons, and time windows. Use regression tests to prevent silent metric degradation.
Always connect metrics to operational consequences: latency budgets, cost constraints, and business impact.
        `.trim()
      }
    ];

    // Evaluation set (deterministic)
    const EVAL_SET = [
      {
        id: "eval-01-monitoring",
        name: "Monitoring basics",
        query: "What should be monitored for deployed ML models? Provide citations.",
        expected: { should_refuse: false, must_retrieve: ["doc-ml-monitoring"], min_top_sim: 0.13 }
      },
      {
        id: "eval-02-cicd",
        name: "CI/CD for ML",
        query: "Explain CI/CD for ML deployment and why model artefact versioning matters. Cite sources.",
        expected: { should_refuse: false, must_retrieve: ["doc-cicd-ml"], min_top_sim: 0.13 }
      },
      {
        id: "eval-03-drift",
        name: "Drift detection",
        query: "What is data drift vs prediction drift, and how do you detect them? Provide citations.",
        expected: { should_refuse: false, must_retrieve: ["doc-drift"], min_top_sim: 0.13 }
      },
      {
        id: "eval-04-feature-store",
        name: "Feature store",
        query: "What is a feature store and when would you use one? Provide citations.",
        expected: { should_refuse: false, must_retrieve: ["doc-feature-store"], min_top_sim: 0.13 }
      },
      {
        id: "eval-05-prompt-injection",
        name: "RAG guardrails",
        query: "Describe a safe RAG workflow with guardrails against prompt injection. Cite sources.",
        expected: { should_refuse: false, must_retrieve: ["doc-rag-guardrails"], min_top_sim: 0.13 }
      },
      {
        id: "eval-06-negative",
        name: "Negative control",
        query: "Give me a detailed recipe for lasagne, step by step.",
        expected: { should_refuse: true, must_retrieve: [], min_top_sim: 0.13 }
      }
    ];

    // Simple tokeniser
    function tokens(s){
      return (s || "")
        .toLowerCase()
        .replace(/[^a-z0-9\s\-]/g, " ")
        .split(/\s+/)
        .filter(t => t && t.length > 2);
    }

    // Build TF-IDF-ish weights
    const DOCS = CORPUS.map(d => {
      const t = tokens(d.text + " " + d.title);
      const tf = new Map();
      for (const w of t) tf.set(w, (tf.get(w) || 0) + 1);
      return { ...d, tf, len: t.length };
    });

    const DF = new Map();
    for (const d of DOCS){
      for (const w of new Set(d.tf.keys())){
        DF.set(w, (DF.get(w) || 0) + 1);
      }
    }

    function idf(w){
      const df = DF.get(w) || 0;
      const N = DOCS.length;
      return Math.log((N + 1) / (df + 1)) + 1.0;
    }

    function dot(a, b){
      let s = 0;
      for (const [k, v] of a){
        const bv = b.get(k);
        if (bv) s += v * bv;
      }
      return s;
    }
    function norm(a){
      let s = 0;
      for (const [, v] of a) s += v * v;
      return Math.sqrt(s) || 1;
    }

    function toTfidfVectorFromTokens(toks){
      const tf = new Map();
      for (const w of toks) tf.set(w, (tf.get(w) || 0) + 1);
      const v = new Map();
      for (const [w, c] of tf){
        v.set(w, (c / toks.length) * idf(w));
      }
      return v;
    }

    // Precompute doc vectors
    for (const d of DOCS){
      const toks = tokens(d.text + " " + d.title);
      d.vec = toTfidfVectorFromTokens(toks);
      d.n = norm(d.vec);
    }

    function retrieve(query, k=3){
      const qtoks = tokens(query);
      const qvec = toTfidfVectorFromTokens(qtoks);
      const qn = norm(qvec);

      const scored = DOCS.map(d => {
        const sim = dot(qvec, d.vec) / (qn * d.n);
        return { doc: d, sim };
      }).sort((a,b)=> b.sim - a.sim);

      const top = scored.slice(0, k);
      const maxSim = top.length ? top[0].sim : 0;
      return { top, maxSim };
    }

    function pickSnippets(text, query){
      const qset = new Set(tokens(query));
      const sentences = text
        .replace(/\s+/g, " ")
        .split(/(?<=[\.\!\?])\s+/)
        .map(s => s.trim())
        .filter(Boolean);

      const ranked = sentences.map(s => {
        const ts = tokens(s);
        let hit = 0;
        for (const w of ts) if (qset.has(w)) hit += 1;
        return { s, hit };
      }).sort((a,b)=> b.hit - a.hit);

      const chosen = ranked.filter(r => r.hit > 0).slice(0, 2).map(r => r.s);
      if (chosen.length) return chosen;

      return [text.slice(0, 180) + (text.length > 180 ? "…" : "")];
    }

    function buildAnswer(query, hits, policy){
      const top = hits.map((h, i) => {
        const snippets = pickSnippets(h.doc.text, query);
        return { i: i+1, title: h.doc.title, url: h.doc.url, snippets };
      });

      if (!top.length){
        return { html: `<p class="muted" style="margin:0;">No sources available.</p>`, cites: "" };
      }

      const cautious = policy === "lenient";

      const bullets = [];
      for (const t of top){
        const s1 = t.snippets[0] || "";
        bullets.push(`<li>${escapeHtml(s1)} <span class="mono">[${t.i}]</span></li>`);
      }

      const preface = cautious
        ? `<p style="margin:0 0 10px;"><strong>Draft (cautious):</strong> evidence is limited; conclusions are bounded by retrieved sources.</p>`
        : `<p style="margin:0 0 10px;"><strong>Grounded draft:</strong> constructed strictly from retrieved sources.</p>`;

      const html = `
        ${preface}
        <ul class="clean">${bullets.join("")}</ul>
        <p class="tiny" style="margin-top:10px;">
          Citations: ${top.map(t => `<span class="mono">[${t.i}]</span> ${escapeHtml(t.title)}`).join(" · ")}
        </p>
      `.trim();

      const citeText = top.map(t => `[${t.i}] ${t.title} — ${t.url}`).join("\n");
      return { html, cites: citeText };
    }

    function escapeHtml(s){
      return (s || "").replace(/[&<>"']/g, m => ({
        "&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#039;"
      }[m]));
    }

    function toast(msg){
      const el = document.getElementById("toast");
      el.textContent = msg;
      el.classList.add("show");
      setTimeout(()=> el.classList.remove("show"), 1400);
    }

    function currentPolicy(){
      const v = document.querySelector('input[name="policy"]:checked')?.value || "strict";
      return v;
    }

    function strictThreshold(){
      return 0.13;
    }

    function renderTrace(hits){
      const trace = document.getElementById("trace");
      trace.innerHTML = "";

      if (!hits.length){
        trace.innerHTML = `<div class="panel"><p class="muted" style="margin:0;">No retrieved sources.</p></div>`;
        return;
      }

      const max = Math.max(...hits.map(h => h.sim), 1e-9);

      for (let idx=0; idx<hits.length; idx++){
        const h = hits[idx];
        const rank = idx + 1;
        const pct = Math.max(0, Math.min(100, (h.sim / max) * 100));
        const snippets = pickSnippets(h.doc.text, document.getElementById("q").value);

        const hit = document.createElement("div");
        hit.className = "hit";
        hit.innerHTML = `
          <div class="hitTop">
            <div>
              <strong class="mono">[${rank}]</strong> ${escapeHtml(h.doc.title)}
              <div class="tiny">${escapeHtml(h.doc.source)} · <a href="${h.doc.url}" class="mono">open</a></div>
            </div>
            <div class="row">
              <div class="tiny mono">sim ${h.sim.toFixed(3)}</div>
              <div class="scoreBar" aria-label="Similarity score">
                <div class="scoreFill" style="width:${pct.toFixed(0)}%"></div>
              </div>
            </div>
          </div>
          <div class="tiny" style="margin-top:8px;">
            ${snippets.map(s => `<div>• ${escapeHtml(s)}</div>`).join("")}
          </div>
        `;
        trace.appendChild(hit);
      }
    }

    let lastTrace = null;
    let lastCitations = "";
    let lastEvalReport = null;

    function run(){
      const q = document.getElementById("q").value.trim();
      const policy = currentPolicy();

      if (!q){
        toast("Please enter a query.");
        return;
      }

      const { top, maxSim } = retrieve(q, 3);
      const threshold = strictThreshold();

      if (policy === "strict" && maxSim < threshold){
        document.getElementById("answer").innerHTML = `
          <p style="margin:0 0 8px;"><strong>Refusal (Strict policy):</strong> evidence is too weak to answer safely.</p>
          <p class="tiny" style="margin:0;">
            Try rephrasing, add more context, or use <span class="kbd">Lenient</span> to get a cautious draft.
            <span class="mono">top similarity=${maxSim.toFixed(3)}</span>, threshold=${threshold.toFixed(3)}.
          </p>
        `;
        renderTrace(top);
        document.getElementById("traceBox").open = true;

        lastTrace = buildTraceJson(q, policy, top, maxSim, threshold, true);
        lastCitations = buildCitations(top);
        return;
      }

      const answer = buildAnswer(q, top, policy);
      document.getElementById("answer").innerHTML = answer.html;
      renderTrace(top);
      document.getElementById("traceBox").open = true;

      lastTrace = buildTraceJson(q, policy, top, maxSim, threshold, false);
      lastCitations = answer.cites;

      toast("Draft generated with retrieval trace.");
    }

    function buildCitations(top){
      return top.map((h, i) => `[${i+1}] ${h.doc.title} — ${h.doc.url}`).join("\n");
    }

    function buildTraceJson(q, policy, top, maxSim, threshold, refused){
      const now = new Date().toISOString();
      return {
        schema_version: "nil.rag.trace.v1",
        generated_at: now,
        policy,
        refused,
        thresholds: { strict_similarity_threshold: threshold },
        query: q,
        retrieval: {
          top_k: top.map((h, i) => ({
            rank: i+1,
            doc_id: h.doc.id,
            title: h.doc.title,
            source: h.doc.source,
            url: h.doc.url,
            similarity: Number(h.sim.toFixed(6)),
            snippets: pickSnippets(h.doc.text, q)
          })),
          top_similarity: Number(maxSim.toFixed(6))
        }
      };
    }

    async function copyTrace(){
      if (!lastTrace){
        toast("Run a query first.");
        return;
      }
      const json = JSON.stringify(lastTrace, null, 2);
      try {
        await navigator.clipboard.writeText(json);
        toast("Trace JSON copied to clipboard.");
      } catch {
        toast("Clipboard blocked by browser.");
      }
    }

    async function copyCitations(){
      if (!lastCitations){
        toast("Run a query first.");
        return;
      }
      try {
        await navigator.clipboard.writeText(lastCitations);
        toast("Citations copied.");
      } catch {
        toast("Clipboard blocked by browser.");
      }
    }

    async function copyEval(){
      if (!lastEvalReport){
        toast("Run an evaluation first.");
        return;
      }
      const json = JSON.stringify(lastEvalReport, null, 2);
      try {
        await navigator.clipboard.writeText(json);
        toast("Eval report JSON copied.");
      } catch {
        toast("Clipboard blocked by browser.");
      }
    }

    function clearAll(){
      document.getElementById("q").value = "";
      document.getElementById("answer").innerHTML = `<p class="muted" style="margin:0;">Cleared. Run a query to generate a grounded draft.</p>`;
      document.getElementById("trace").innerHTML = "";
      document.getElementById("traceBox").open = false;
      lastTrace = null;
      lastCitations = "";
      toast("Cleared.");
    }

    // -----------------------------
    // Evaluation Harness
    // -----------------------------

    function setEvalPills({overall, refusal, retrieval, citations, trace}){
      document.getElementById("evalScore").textContent = overall;
      document.getElementById("evalRefusal").textContent = refusal;
      document.getElementById("evalRetrieval").textContent = retrieval;
      document.getElementById("evalCitations").textContent = citations;
      document.getElementById("evalTrace").textContent = trace;
    }

    function fmtPct(x){
      const v = Math.round(x * 100);
      return `${v}%`;
    }

    function runEvaluation(policy){
      const threshold = strictThreshold();
      const rows = [];
      let okRefusal = 0, totRefusal = 0;
      let okRetrieval = 0, totRetrieval = 0;
      let okCites = 0, totCites = 0;
      let okTrace = 0, totTrace = 0;

      for (const t of EVAL_SET){
        const { top, maxSim } = retrieve(t.query, 3);
        const shouldRefuse = t.expected.should_refuse === true;

        const refused = (policy === "strict" && maxSim < threshold);
        totRefusal += 1;
        const refusalCorrect = (shouldRefuse && refused) || (!shouldRefuse && !refused);
        if (refusalCorrect) okRefusal += 1;

        // Retrieval@k proxy: must_retrieve docs appear in top-k doc_id list (for positive tests)
        const topIds = top.map(h => h.doc.id);
        let retrievalOk = true;
        if (t.expected.must_retrieve && t.expected.must_retrieve.length){
          totRetrieval += 1;
          retrievalOk = t.expected.must_retrieve.every(id => topIds.includes(id));
          if (retrievalOk) okRetrieval += 1;
        }

        // Citation presence: if not refused, answer should include at least one [1] style citation
        totCites += 1;
        let citesOk = true;
        let citesText = "";
        if (!refused){
          const answer = buildAnswer(t.query, top, policy);
          citesText = answer.cites || "";
          citesOk = /\[\d+\]/.test(answer.html);
        } else {
          // in refusal we still allow citations to be empty; keep it "ok" if refused correctly
          citesOk = refusalCorrect;
        }
        if (citesOk) okCites += 1;

        // Trace completeness: must have top_k entries and include similarity + snippets
        totTrace += 1;
        const traceJson = buildTraceJson(t.query, policy, top, maxSim, threshold, refused);
        const traceOk =
          Array.isArray(traceJson.retrieval?.top_k) &&
          traceJson.retrieval.top_k.length > 0 &&
          typeof traceJson.retrieval.top_k[0].similarity === "number" &&
          Array.isArray(traceJson.retrieval.top_k[0].snippets);
        if (traceOk) okTrace += 1;

        const expectedLabel = shouldRefuse ? "Refuse" : "Answer";
        const refusalLabel = refused ? `<span class="ok">REFUSED</span>` : `<span class="warn">ANSWERED</span>`;

        const simLabel = maxSim.toFixed(3);
        const notes = [];
        if (policy === "strict" && !shouldRefuse && refused) notes.push("Over-refusal risk");
        if (policy === "strict" && shouldRefuse && !refused) notes.push("Under-refusal risk");
        if (t.expected.must_retrieve?.length && !retrievalOk) notes.push("Missed expected source");
        if (!citesOk) notes.push("No citations in answer");
        if (!traceOk) notes.push("Trace incomplete");

        rows.push({
          name: t.name,
          query: t.query,
          expected: expectedLabel,
          refused,
          refusalCorrect,
          retrievalOk: t.expected.must_retrieve?.length ? retrievalOk : null,
          citesOk,
          maxSim,
          notes: notes.join("; ")
        });
      }

      const refusalRate = okRefusal / Math.max(1, totRefusal);
      const retrievalRate = okRetrieval / Math.max(1, totRetrieval || 1); // avoid div0
      const citeRate = okCites / Math.max(1, totCites);
      const traceRate = okTrace / Math.max(1, totTrace);

      // Overall score weighting: refusal 40%, retrieval 25%, citations 20%, trace 15%
      const overall =
        0.40 * refusalRate +
        0.25 * retrievalRate +
        0.20 * citeRate +
        0.15 * traceRate;

      renderEvalTable(rows, policy);

      setEvalPills({
        overall: fmtPct(overall),
        refusal: fmtPct(refusalRate),
        retrieval: fmtPct(retrievalRate),
        citations: fmtPct(citeRate),
        trace: fmtPct(traceRate)
      });

      lastEvalReport = {
        schema_version: "nil.rag.eval.v1",
        generated_at: new Date().toISOString(),
        policy,
        strict_similarity_threshold: threshold,
        corpus_size: DOCS.length,
        top_k: 3,
        scores: {
          overall: Number(overall.toFixed(6)),
          refusal_correctness: Number(refusalRate.toFixed(6)),
          retrieval_at_k: Number(retrievalRate.toFixed(6)),
          citation_presence: Number(citeRate.toFixed(6)),
          trace_completeness: Number(traceRate.toFixed(6))
        },
        results: rows.map(r => ({
          test: r.name,
          query: r.query,
          expected: r.expected,
          refused: r.refused,
          refusal_correct: r.refusalCorrect,
          retrieval_ok: r.retrievalOk,
          citations_ok: r.citesOk,
          top_similarity: Number(r.maxSim.toFixed(6)),
          notes: r.notes
        }))
      };

      toast(`Evaluation complete (${policy}).`);
    }

    function renderEvalTable(rows, policy){
      const body = document.getElementById("evalBody");
      body.innerHTML = "";

      for (const r of rows){
        const tr = document.createElement("tr");

        const refusalOkClass = r.refusalCorrect ? "ok" : "bad";
        const refusalTxt = r.refused ? "REFUSED" : "ANSWERED";

        let retrievalCell = "—";
        if (r.retrievalOk === null){
          retrievalCell = "n/a";
        } else {
          retrievalCell = r.retrievalOk ? `<span class="ok">OK</span>` : `<span class="bad">MISS</span>`;
        }

        const citesCell = r.citesOk ? `<span class="ok">OK</span>` : `<span class="bad">NO</span>`;
        const simCell = `<span class="mono">${r.maxSim.toFixed(3)}</span>`;

        tr.innerHTML = `
          <td><strong>${escapeHtml(r.name)}</strong></td>
          <td>${escapeHtml(r.query)}</td>
          <td><span class="mono">${escapeHtml(r.expected)}</span></td>
          <td><span class="${refusalOkClass}">${refusalTxt}</span></td>
          <td>${retrievalCell}</td>
          <td>${citesCell}</td>
          <td>${simCell}</td>
          <td class="tiny">${escapeHtml(r.notes || "")}</td>
        `;
        body.appendChild(tr);
      }
    }

    // -----------------------------
    // Wire up UI
    // -----------------------------

    document.getElementById("run").addEventListener("click", run);
    document.getElementById("clear").addEventListener("click", clearAll);
    document.getElementById("copyTrace").addEventListener("click", copyTrace);
    document.getElementById("copyCites").addEventListener("click", copyCitations);
    document.getElementById("copyEval").addEventListener("click", copyEval);

    document.getElementById("runEvalStrict").addEventListener("click", () => runEvaluation("strict"));
    document.getElementById("runEvalLenient").addEventListener("click", () => runEvaluation("lenient"));

    document.getElementById("q").addEventListener("keydown", (e) => {
      if (e.key === "Enter"){
        e.preventDefault();
        run();
      }
    });

    for (const btn of document.querySelectorAll("[data-example]")){
      btn.addEventListener("click", () => {
        document.getElementById("q").value = btn.getAttribute("data-example") || "";
        toast("Example query loaded.");
      });
    }

    // Initialise eval pills
    setEvalPills({ overall: "—", refusal: "—", retrieval: "—", citations: "—", trace: "—" });
  </script>
</body>
</html>
