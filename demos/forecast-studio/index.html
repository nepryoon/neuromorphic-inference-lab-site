<!doctype html>
<html lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Forecast Studio — Projects</title>
  <meta name="description" content="Forecast Studio: a time-series forecasting system with backtesting, feature pipelines, MLOps patterns, and deployment-ready outputs." />
  <link rel="icon" href="/favicon.svg" />
  <link rel="stylesheet" href="/style.css" />
</head>

<body>
  <header class="nav">
    <div class="inner">
      <div class="brand">
        <img src="/logo.svg" alt="Neuromorphic Inference Lab" />
        <span>Neuromorphic Inference Lab</span>
      </div>
      <nav class="navlinks" aria-label="Primary navigation">
        <a data-nav href="/">Home</a>
        <a data-nav class="active" href="/demos/">Projects</a>
        <a data-nav href="/about/">About Me</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="hero">
      <p class="kicker">Project</p>
      <h1 class="h1">Forecast Studio</h1>
      <p class="sub">
        Forecasting treated as engineering: reproducible data pipelines, backtesting harness, versioned artefacts, and deployment-ready outputs.
        Designed to evidence full-stack ML (Data → Model → Production).
      </p>

      <div class="badges">
        <span class="badge">Time-Series</span>
        <span class="badge">Feature Pipelines</span>
        <span class="badge">Backtesting</span>
        <span class="badge">Model Registry</span>
        <span class="badge">CI/CD for ML</span>
      </div>

      <div style="height:14px"></div>

      <div class="actions">
        <a class="btn primary" href="https://github.com/nepryoon/nil-forecast-studio" target="_blank" rel="noopener">Source repository</a>
        <a class="btn" href="/demos/">Back to Projects</a>
      </div>
    </section>

    <section class="section">
      <h2>Overview — For Recruiters</h2>
      <p class="lead">
        Forecast Studio is a time-series forecasting system that turns raw historical data into production-ready predictions.
        Instead of one-off notebooks that require manual intervention, it provides an automated pipeline that ingests data,
        generates features, trains models, evaluates accuracy, and outputs forecasts that can be used by downstream systems.
      </p>

      <p class="lead">
        The system solves a common problem in data science: moving from exploratory analysis to reliable, repeatable forecasts.
        Many teams struggle to operationalize forecasting because notebooks don't enforce data validation, feature pipelines drift
        as requirements change, and there's no systematic way to test accuracy before deployment. Forecast Studio addresses these
        gaps by treating forecasting as an engineering discipline with defined stages, version control, and quality gates.
      </p>

      <p class="lead">
        This project demonstrates end-to-end machine learning system design. It shows how to structure data pipelines for
        reproducibility, implement backtesting to prevent overfitting, manage model artifacts across iterations, and prepare
        outputs for production deployment. The architecture follows MLOps principles: every step is automated, versioned, and
        designed to run without human intervention once configured.
      </p>
    </section>

    <section class="section">
      <h2>Technical Deep-Dive — For Engineers</h2>

      <h3>Purpose and Scope</h3>
      <p class="lead">
        Forecast Studio implements a full-cycle time-series ML system: data ingestion, feature engineering, model training,
        backtesting evaluation, and artifact publishing. The scope is deliberately constrained to univariate and simple
        multivariate forecasting using classical statistical methods (ARIMA, exponential smoothing, Prophet) and gradient
        boosting (LightGBM, XGBoost). This constraint allows focus on pipeline architecture and MLOps patterns rather than
        bleeding-edge modeling techniques.
      </p>

      <h3>Architecture and Design Decisions</h3>
      <p class="lead">
        The system follows a modular pipeline architecture with five stages: Ingest, Transform, Train, Evaluate, Publish.
        Each stage is isolated with defined input/output contracts, making it possible to swap implementations without
        affecting downstream components. Data flows through immutable transformations: raw input is never modified in place,
        each transformation produces versioned output, and all intermediate artifacts are stored for debugging and auditing.
      </p>

      <p class="lead">
        Design decisions prioritize reproducibility and testability. Configuration is declarative (YAML/JSON) and separated
        from code. Feature pipelines are deterministic: given the same raw data and config, they produce identical output.
        The backtesting harness uses walk-forward validation with fixed time splits to simulate real deployment conditions.
        Model selection happens automatically via cross-validation, but results are logged with full parameter provenance.
      </p>

      <h3>Data Flow and Execution Model</h3>
      <p class="lead">
        Execution follows a directed acyclic graph (DAG). The ingest stage validates schema and handles missing values using
        configurable strategies (forward-fill, interpolation, sentinel). The transform stage generates lag features, rolling
        window statistics, and calendar-based features (day-of-week, month, holidays). Feature generation is lag-aware:
        forecasts use only information available at prediction time to prevent leakage.
      </p>

      <p class="lead">
        The train stage fits multiple candidate models in parallel, each with hyperparameter search. The evaluate stage runs
        backtesting: for each time split, the model is trained on history, forecasts the holdout period, and metrics (MAE, RMSE,
        MAPE) are aggregated. The publish stage serializes the best model, writes forecasts in a standard format (CSV with
        timestamp + value + confidence interval), and stores metadata (training date, data version, hyperparameters).
      </p>

      <h3>Technology Stack and Rationale</h3>
      <p class="lead">
        <strong>Languages:</strong> Python 3.10+ for core implementation. Python was chosen for its mature ecosystem of
        time-series libraries (statsmodels, Prophet, scikit-learn) and data manipulation tools (pandas, NumPy). Type hints
        (mypy) are used throughout to catch errors at development time.
      </p>

      <p class="lead">
        <strong>Frameworks:</strong> Prefect for workflow orchestration. Prefect provides task retries, parameter management,
        and execution logging without requiring cluster infrastructure. Compared to Airflow, Prefect has simpler local setup
        and better support for dynamic workflows where the task graph depends on runtime data.
      </p>

      <p class="lead">
        <strong>Libraries:</strong> pandas for data manipulation (ubiquitous, well-tested, handles time-series indexing),
        statsmodels for classical forecasting (ARIMA, SARIMAX, exponential smoothing), Prophet for additive models with
        seasonality, LightGBM for gradient boosting (faster training than XGBoost, better handling of categorical features),
        scikit-learn for preprocessing and cross-validation utilities, pydantic for configuration validation (runtime schema
        checks prevent config errors).
      </p>

      <p class="lead">
        <strong>Tools:</strong> pytest for testing (fixtures, parametrization, coverage reporting), black for code formatting
        (removes formatting debates), ruff for linting (faster than flake8, enforces import order and naming conventions),
        pre-commit for git hooks (runs checks before commit to catch issues early).
      </p>

      <p class="lead">
        <strong>Infrastructure:</strong> Docker for containerization (ensures consistent environment across local and production),
        GitHub Actions for CI/CD (runs tests on every push, automates versioning), DVC (Data Version Control) for tracking
        datasets and models (Git for code, DVC for large files, keeps repository lightweight). MLflow for experiment tracking
        (logs parameters, metrics, and artifacts for every training run).
      </p>

      <p class="lead">
        <strong>MLOps Patterns:</strong> Model registry (MLflow) stores trained models with version tags, making rollback
        straightforward. Feature store pattern is implemented via versioned Parquet files (simpler than dedicated feature stores
        like Feast for this scope). Monitoring hooks are built in: the system logs prediction distributions and feature statistics
        at inference time, enabling drift detection downstream.
      </p>

      <h3>Technical Challenges and Trade-offs</h3>
      <p class="lead">
        <strong>Feature leakage prevention:</strong> The main challenge in time-series ML is ensuring forecasts use only past
        information. The solution is lag-aware feature engineering: all rolling windows and lags are computed with explicit
        offsets, and unit tests verify that forecast-time features match training-time features for corresponding timestamps.
      </p>

      <p class="lead">
        <strong>Hyperparameter tuning cost:</strong> Grid search over multiple models and splits is computationally expensive.
        The trade-off is between search thoroughness and runtime. The system uses randomized search with early stopping (Optuna)
        rather than exhaustive grid search, which provides 80% of the benefit at 20% of the cost. For production, hyperparameters
        are cached and reused unless data distribution changes significantly.
      </p>

      <p class="lead">
        <strong>Model selection strategy:</strong> Choosing between classical methods (ARIMA) and gradient boosting (LightGBM)
        depends on data characteristics. ARIMA works well for stationary series with clear autoregressive patterns, while LightGBM
        handles non-linear relationships and exogenous features better. The system fits both and selects based on backtesting
        performance, but this increases training time. The trade-off is runtime cost vs. robustness to varied data.
      </p>

      <p class="lead">
        <strong>Deployment output format:</strong> Forecasts are written as CSV with explicit confidence intervals. CSV was
        chosen over database writes or API endpoints for simplicity: downstream teams can ingest files without coordinating
        credentials or schemas. The trade-off is lack of real-time capability: this system is designed for batch forecasting
        (hourly, daily) rather than sub-second inference.
      </p>
    </section>

    <section class="section">
      <h2>Interactive mini-demo (local, no backend)</h2>
      <p class="lead">
        This is a lightweight in-browser illustration: select a horizon and compare a baseline forecast to the last observed trend.
        It exists to show system thinking, not to replace the production pipeline.
      </p>

      <div class="card">
        <div class="demo">
          <div>
            <label class="small" for="h">Forecast horizon (steps)</label>
            <select id="h" class="select">
              <option value="6">6</option>
              <option value="12" selected>12</option>
              <option value="18">18</option>
            </select>

            <div style="height:12px"></div>

            <label class="small" for="mode">Baseline method</label>
            <select id="mode" class="select">
              <option value="naive">Naïve (last value)</option>
              <option value="ma">Moving average (window=3)</option>
              <option value="linear">Linear trend</option>
              <option value="holt">Exponential smoothing (Holt)</option>
              <option value="seasonal" selected>Seasonal decomposition</option>
            </select>

            <div style="height:14px"></div>

            <div class="actions">
              <button id="run" class="btn primary" type="button">Run forecast</button>
              <button id="reset" class="btn" type="button">Reset</button>
            </div>

            <div style="height:14px"></div>
            <div class="panel">
              <div class="small">Output</div>
              <div id="out" style="margin-top:8px; line-height:1.6; color: rgba(234,240,255,.86);">
                Horizon: 12 · Baseline: Seasonal decomposition · MAE (demo split): —
              </div>
            </div>
          </div>

          <div>
            <div class="panel">
              <div class="small">Series preview</div>
              <div style="height:10px"></div>
              <svg id="chart" viewBox="0 0 700 260" width="100%" height="260" role="img" aria-label="Time-series chart"></svg>
              <div class="small" style="margin-top:10px;">
                Solid = observed · Dashed = forecast (demo)
              </div>
            </div>
          </div>
        </div>

        <div style="height:12px"></div>

        <details>
          <summary class="small" style="cursor:pointer;">Demo data (generated)</summary>
          <pre id="dataDump"></pre>
        </details>
      </div>
    </section>

    <footer class="footer">
      <div>© 2025 Neuromorphic Inference Lab</div>
      <div class="prov">
        <span id="build-branch">branch: …</span>
        <span id="build-commit">commit: …</span>
        <span id="build-time">built: …</span>
      </div>
    </footer>
  </main>

  <script>

    // ---- Demo data (generated locally) ----
    const N = 42;
    const series = Array.from({length: N}, (_, i) => {
      const t = i;
      const trend  = 0.55 * t;
      const season = 6 * Math.sin((2 * Math.PI * t) / 12);
      const noise  = (Math.sin(t * 1.7) + Math.cos(t * 0.9)) * 0.9;
      return Math.round((80 + trend + season + noise) * 10) / 10;
    });

    document.getElementById("dataDump").textContent = JSON.stringify(series, null, 2);

    const svg = document.getElementById("chart");

    function forecast(arr, horizon, mode) {
      const n = arr.length;

      if (mode === "naive") {
        // Naïve with drift: extends the overall trend from first to last point
        const drift = (arr[n - 1] - arr[0]) / (n - 1);
        return Array.from({length: horizon}, (_, i) =>
          Math.round((arr[n - 1] + drift * (i + 1)) * 10) / 10
        );
      }

      if (mode === "ma") {
        // Moving average with trend estimated from last window
        const w = 3;
        const slice = arr.slice(-w);
        const avg = slice.reduce((s, x) => s + x, 0) / slice.length;
        const trend = (arr[n - 1] - arr[n - w]) / (w - 1);
        return Array.from({length: horizon}, (_, i) =>
          Math.round((avg + trend * (i + 1)) * 10) / 10
        );
      }

      if (mode === "linear") {
        // Ordinary least squares on the full series
        const xs  = Array.from({length: n}, (_, i) => i);
        const mx  = xs.reduce((s, x) => s + x, 0) / n;
        const my  = arr.reduce((s, x) => s + x, 0) / n;
        const num = xs.reduce((s, x, i) => s + (x - mx) * (arr[i] - my), 0);
        const den = xs.reduce((s, x) => s + (x - mx) ** 2, 0);
        const slope     = den ? num / den : 0;
        const intercept = my - slope * mx;
        return Array.from({length: horizon}, (_, i) =>
          Math.round((intercept + slope * (n + i)) * 10) / 10
        );
      }

      if (mode === "holt") {
        // Holt double exponential smoothing (level + trend)
        const alpha = 0.35, beta = 0.25;
        let l = arr[0];
        let b = arr[1] - arr[0];
        for (let i = 1; i < n; i++) {
          const lPrev = l;
          l = alpha * arr[i] + (1 - alpha) * (l + b);
          b = beta  * (l - lPrev) + (1 - beta) * b;
        }
        return Array.from({length: horizon}, (_, i) =>
          Math.round((l + b * (i + 1)) * 10) / 10
        );
      }

      if (mode === "seasonal") {
        const period = 12;
        const half   = Math.floor(period / 2);

        // Step 1: centered moving average to extract trend
        const trendArr = new Array(n).fill(null);
        for (let i = half; i < n - half; i++) {
          const slice = arr.slice(i - half, i + half + 1);
          trendArr[i] = slice.reduce((s, x) => s + x, 0) / slice.length;
        }

        // Step 2: seasonal indices — average detrended residual per cycle position
        const buckets = Array.from({length: period}, () => []);
        for (let i = 0; i < n; i++) {
          if (trendArr[i] !== null) {
            buckets[i % period].push(arr[i] - trendArr[i]);
          }
        }
        const seasonal = buckets.map(b =>
          b.length ? b.reduce((s, x) => s + x, 0) / b.length : 0
        );

        // Step 3: linear regression on valid trend values to extrapolate
        const tValid = trendArr.filter(v => v !== null);
        const tIdx   = trendArr.map((v, i) => v !== null ? i : null).filter(v => v !== null);
        const tm  = tIdx.reduce((s, x) => s + x, 0) / tIdx.length;
        const vm  = tValid.reduce((s, x) => s + x, 0) / tValid.length;
        const num = tIdx.reduce((s, x, i) => s + (x - tm) * (tValid[i] - vm), 0);
        const den = tIdx.reduce((s, x) => s + (x - tm) ** 2, 0);
        const slope     = den ? num / den : 0;
        const intercept = vm - slope * tm;

        // Step 4: forecast = extrapolated trend + seasonal pattern
        return Array.from({length: horizon}, (_, i) => {
          const futureIdx = n + i;
          const trendVal  = intercept + slope * futureIdx;
          const seasVal   = seasonal[futureIdx % period];
          return Math.round((trendVal + seasVal) * 10) / 10;
        });
      }

      // fallback
      return Array.from({length: horizon}, () => arr[n - 1]);
    }

    function mae(yTrue, yPred) {
      const n = Math.min(yTrue.length, yPred.length);
      let s = 0;
      for (let i = 0; i < n; i++) s += Math.abs(yTrue[i] - yPred[i]);
      return n ? s / n : 0;
    }

    function renderChart(obs, pred) {
      const W = 700, H = 260;
      const padL = 40, padR = 16, padT = 16, padB = 28;

      const all = obs.concat(pred);
      const min = Math.min(...all) - 3;
      const max = Math.max(...all) + 3;

      const totalPts = obs.length + (pred.length > 0 ? pred.length : 1);
      const xObs  = i => padL + (i  * (W - padL - padR)) / (totalPts - 1);
      const xPred = i => xObs(obs.length - 1 + i);
      const y     = v => padT + (H - padT - padB) * (1 - (v - min) / (max - min));
      const path  = pts => pts.map((p, i) => `${i === 0 ? "M" : "L"} ${p[0].toFixed(2)} ${p[1].toFixed(2)}`).join(" ");

      const obsPts  = obs.map((v, i) => [xObs(i), y(v)]);
      const predPts = pred.map((v, i) => [xPred(i), y(v)]);

      const gridLines = 4;
      const grid = Array.from({length: gridLines + 1}, (_, i) => {
        const yy = padT + (i * (H - padT - padB)) / gridLines;
        return `<line x1="${padL}" y1="${yy}" x2="${W - padR}" y2="${yy}" stroke="rgba(255,255,255,.08)" />`;
      }).join("");

      const axis = `
        <line x1="${padL}" y1="${H - padB}" x2="${W - padR}" y2="${H - padB}" stroke="rgba(255,255,255,.14)" />
        <line x1="${padL}" y1="${padT}"     x2="${padL}"     y2="${H - padB}" stroke="rgba(255,255,255,.14)" />
      `;

      const obsPath  = `<path d="${path(obsPts)}" fill="none" stroke="rgba(110,231,255,.90)" stroke-width="2.4" />`;
      const predPath = pred.length
        ? `<path d="${path([[obsPts[obsPts.length - 1][0], obsPts[obsPts.length - 1][1]]].concat(predPts))}"
             fill="none" stroke="rgba(167,139,250,.92)" stroke-width="2.4" stroke-dasharray="7 7" />`
        : "";

      const legend = `
        <g>
          <rect x="${padL}"       y="${H - 22}" width="12" height="3" fill="rgba(110,231,255,.90)"></rect>
          <text x="${padL + 18}"  y="${H - 18}" fill="rgba(234,240,255,.75)" font-size="12">observed</text>
          <rect x="${padL + 90}"  y="${H - 22}" width="12" height="3" fill="rgba(167,139,250,.92)"></rect>
          <text x="${padL + 108}" y="${H - 18}" fill="rgba(234,240,255,.75)" font-size="12">forecast</text>
        </g>
      `;

      svg.innerHTML = `
        <rect x="0" y="0" width="${W}" height="${H}" rx="14" ry="14" fill="rgba(255,255,255,.02)" />
        ${grid}${axis}${obsPath}${predPath}${legend}
      `;
    }

    function run() {
      const horizon = parseInt(document.getElementById("h").value, 10);
      const mode    = document.getElementById("mode").value;

      const split    = series.length - 8;
      const train    = series.slice(0, split);
      const test     = series.slice(split);
      const predTest = forecast(train, test.length, mode);
      const demoMae  = mae(test, predTest);
      const pred     = forecast(series, horizon, mode);

      renderChart(series, pred);

      const labels = {
        naive:    "Naïve (drift)",
        ma:       "Moving average (trend)",
        linear:   "Linear trend (OLS)",
        holt:     "Exponential smoothing (Holt)",
        seasonal: "Seasonal decomposition"
      };
      document.getElementById("out").textContent =
        `Horizon: ${horizon} · Baseline: ${labels[mode] || mode} · MAE (demo split): ${demoMae.toFixed(2)}`;
    }

    document.getElementById("run").addEventListener("click", run);
    document.getElementById("reset").addEventListener("click", () => {
      document.getElementById("h").value    = "12";
      document.getElementById("mode").value = "seasonal";
      renderChart(series, []);
      document.getElementById("out").textContent =
        "Horizon: 12 · Baseline: Seasonal decomposition · MAE (demo split): —";
    });

    // initial render
    renderChart(series, []);
  </script>

  <script src="/build-info.js"></script>
</body>
</html>
